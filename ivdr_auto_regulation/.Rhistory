webpage <- read_html(url)
# Extract the main text content
text_content <- webpage %>% html_text()
text_content
# Split the text by "Article"
articles <- str_split(text_content, "Article")[[1]]
# Function to clean and save each article
save_articles <- function(article_text, index) {
# Clean the article text
article_text <- str_trim(article_text)
article_text <- paste("Article", article_text)
# Create a file name for each article
file_name <- paste0("Article_", index, ".txt")
# Save the article text to a file
writeLines(article_text, con = file_name)
}
# Loop through each article and save it
for (i in 2:length(articles)) {
save_articles(articles[[i]], i - 1)
}
# Inform the user that the process is complete
cat("Articles have been successfully split and saved to text files.\n")
# Install necessary packages if not already installed
if (!requireNamespace("rvest", quietly = TRUE)) install.packages("rvest")
if (!requireNamespace("stringr", quietly = TRUE)) install.packages("stringr")
# Load necessary libraries
library(rvest)
library(stringr)
# URL of the EU regulation guidance
url <- "https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:02017R0746-20230320"
# Read the webpage content
webpage <- read_html(url)
# Extract the ids of all articles
article_ids <- webpage %>% html_nodes(xpath = "//div[contains(@id, 'art_')]") %>% html_attr("id")
# Function to clean and save each article
save_article <- function(article_html, id) {
# Clean the article text
article_text <- article_html %>% html_text(trim = TRUE)
# Create a file name for each article
file_name <- paste0("Article_", gsub("art_", "", id), ".txt")
# Save the article text to a file
writeLines(article_text, con = file_name)
}
# Loop through each article id, extract content, and save it
for (i in seq_along(article_ids)) {
article_id <- article_ids[i]
next_article_id <- ifelse(i < length(article_ids), article_ids[i + 1], NULL)
# Extract content between the current and next article id
if (is.null(next_article_id)) {
article_content <- webpage %>% html_nodes(xpath = sprintf("//div[@id='%s']", article_id))
} else {
article_content <- webpage %>% html_nodes(xpath = sprintf("//div[@id='%s']/following-sibling::node()[preceding-sibling::div[@id='%s']]", next_article_id, article_id))
}
save_article(article_content, article_id)
}
# Install necessary packages if not already installed
if (!requireNamespace("rvest", quietly = TRUE)) install.packages("rvest")
if (!requireNamespace("stringr", quietly = TRUE)) install.packages("stringr")
# Load necessary libraries
library(rvest)
library(stringr)
# URL of the EU regulation guidance
url <- "https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:02017R0746-20230320"
# Read the webpage content
webpage <- read_html(url)
# Extract the ids of all articles
article_ids <- webpage %>% html_nodes(xpath = "//div[contains(@id, 'art_')]") %>% html_attr("id")
# Function to clean and save each article
save_article <- function(article_html, id) {
# Clean the article text
article_text <- article_html %>% html_text(trim = TRUE)
# Check if article_text is not empty
if (nchar(article_text) > 0) {
# Create a file name for each article
file_name <- paste0("Article_", gsub("art_", "", id), ".txt")
# Save the article text to a file
writeLines(article_text, con = file_name)
} else {
cat("Skipping empty article for ID:", id, "\n")
}
}
# Loop through each article id, extract content, and save it
for (i in seq_along(article_ids)) {
article_id <- article_ids[i]
next_article_id <- ifelse(i < length(article_ids), article_ids[i + 1], NULL)
# Extract content for the current article
if (is.null(next_article_id)) {
article_content <- webpage %>% html_nodes(xpath = sprintf("//*[@id='%s']", article_id))
} else {
article_content <- webpage %>% html_nodes(xpath = sprintf("//*[@id='%s']/following-sibling::div[preceding-sibling::div[@id='%s']]", next_article_id, article_id))
}
# Save the article content if not empty
if (length(article_content) > 0) {
save_article(article_content, article_id)
} else {
cat("No content found for ID:", article_id, "\n")
}
}
# Install necessary packages if not already installed
if (!requireNamespace("rvest", quietly = TRUE)) install.packages("rvest")
if (!requireNamespace("stringr", quietly = TRUE)) install.packages("stringr")
# Load necessary libraries
library(rvest)
library(stringr)
# URL of the EU regulation guidance
url <- "https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:02017R0746-20230320"
# Read the webpage content
webpage <- read_html(url)
# Extract the ids of all articles
article_ids <- webpage %>% html_nodes(xpath = "//div[contains(@class, 'eli-subdivision')]") %>% html_attr("id")
# Function to clean and save each article
save_article <- function(article_html, id) {
# Clean the article text
article_text <- article_html %>% html_text(trim = TRUE)
# Check if article_text is not empty
if (nchar(article_text) > 0) {
# Create a file name for each article
file_name <- paste0("Article_", gsub("art_", "", id), ".txt")
# Save the article text to a file
writeLines(article_text, con = file_name)
} else {
cat("Skipping empty article for ID:", id, "\n")
}
}
# Loop through each article id, extract content, and save it
for (i in seq_along(article_ids)) {
article_id <- article_ids[i]
next_article_id <- ifelse(i < length(article_ids), article_ids[i + 1], NULL)
# Extract content for the current article
if (is.null(next_article_id)) {
article_content <- webpage %>% html_nodes(xpath = sprintf("//*[@id='%s']/following-sibling::*", article_id))
} else {
article_content <- webpage %>% html_nodes(xpath = sprintf("//*[@id='%s']/following-sibling::*[preceding-sibling::div[@id='%s']]", next_article_id, article_id))
}
# Save the article content if not empty
if (length(article_content) > 0) {
save_article(article_content, article_id)
} else {
cat("No content found for ID:", article_id, "\n")
}
}
# Install and load necessary libraries
if (!requireNamespace("rvest", quietly = TRUE)) install.packages("rvest")
if (!requireNamespace("stringr", quietly = TRUE)) install.packages("stringr")
library(rvest)
library(stringr)
# URL of the EU regulation guidance
url <- "https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:02017R0746-20230320"
# Read the webpage content
webpage <- read_html(url)
# Extract all divs with class eli-subdivision and their IDs
article_nodes <- webpage %>% html_nodes("div.eli-subdivision")
# Loop through each article node, extract content, and save it
for (node in article_nodes) {
# Extract the ID of the current article
article_id <- node %>% html_attr("id")
# Extract the title of the article
article_title <- node %>% html_node("p.title-article-norm") %>% html_text(trim = TRUE)
# Extract the content of the article
article_content <- node %>% html_node(xpath = "following-sibling::*[1]") %>% html_text(trim = TRUE)
# Combine the title and content
article_text <- paste(article_title, article_content, sep = "\n\n")
# Create a file name for each article
file_name <- paste0("Article_", gsub("art_", "", article_id), ".txt")
# Save the article text to a file
writeLines(article_text, con = file_name)
}
# Inform the user that the process is complete
cat("Articles have been successfully split and saved to text files.\n")
# Install and load necessary libraries
if (!requireNamespace("rvest", quietly = TRUE)) install.packages("rvest")
if (!requireNamespace("stringr", quietly = TRUE)) install.packages("stringr")
if (!requireNamespace("dplyr", quietly = TRUE)) install.packages("dplyr")
library(rvest)
library(stringr)
library(dplyr)
# Extract all divs with class eli-subdivision and their IDs
article_nodes <- webpage %>% html_nodes("div.eli-subdivision")
# Initialize an empty data frame to store article contents
article_df <- data.frame(article = character(), content = character(), stringsAsFactors = FALSE)
# Loop through each article node, extract content, and save it to the data frame
for (node in article_nodes) {
# Extract the ID of the current article
article_id <- node %>% html_attr("id")
# Extract the title of the article
article_title <- node %>% html_node("p.title-article-norm") %>% html_text(trim = TRUE)
# Extract the content of the article
article_content <- node %>% html_nodes(xpath = ".//following-sibling::*[preceding-sibling::p[@class='title-article-norm']][preceding-sibling::p[@class='title-article-norm'][1][@id='id-6ba2ff5e-29a7-4d58-a664-f868c3fe9904']]") %>% html_text(trim = TRUE)
# Combine the title and content
article_text <- paste(article_title, paste(article_content, collapse = "\n"), sep = "\n\n")
# Add the article number and content to the data frame
article_df <- rbind(article_df, data.frame(article = article_id, content = article_text, stringsAsFactors = FALSE))
}
View(article_df)
# Install and load necessary libraries
if (!requireNamespace("rvest", quietly = TRUE)) install.packages("rvest")
if (!requireNamespace("stringr", quietly = TRUE)) install.packages("stringr")
if (!requireNamespace("dplyr", quietly = TRUE)) install.packages("dplyr")
library(rvest)
library(stringr)
library(dplyr)
# URL of the EU regulation guidance
url <- "https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:02017R0746-20230320"
# Initialize an empty data frame to store article contents
article_df <- data.frame(article = character(), content = character(), stringsAsFactors = FALSE)
# Loop through each article node, extract content, and save it to the data frame
for (node in article_nodes) {
# Extract the ID of the current article
article_id <- node %>% html_attr("id")
# Extract the title of the article
article_title <- node %>% html_node("p.title-article-norm") %>% html_text(trim = TRUE)
# Extract all paragraphs and inline elements within the article
article_content <- node %>% html_nodes(xpath = ".//p | .//div[@class='norm'] | .//span[@class='no-parag']") %>% html_text(trim = TRUE)
# Combine the title and content
article_text <- paste(article_title, paste(article_content, collapse = "\n"), sep = "\n\n")
# Add the article number and content to the data frame
article_df <- rbind(article_df, data.frame(article = article_id, content = article_text, stringsAsFactors = FALSE))
}
View(article_df)
# Save the data frame to a CSV file
write.csv(article_df, "./data/EU_Regulation_Articles.csv", row.names = FALSE)
library(kableExtra)
library(DBI)
library(knitr)
library(dplyr)
library(igraph)
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=60), width=60)
kable <- function(data) {
knitr::kable(data, format="latex", longtable = FALSE, booktabs=FALSE) %>%
kable_styling(latex_options =c("striped", "scale_down", "HOLD_position"))
}
# Install kableExtra package if you haven't
# if (!requireNamespace("kableExtra", quietly = TRUE))
#   install.packages("kableExtra")
# Define global variables for the database connection to ensure it stays open
run_id <- params$run_id
dbname <- "DNAsnake_IVDR.db"
# Connect to the database
db <- dbConnect(RSQLite::SQLite(), dbname = dbname)
# Ensure the database connection is valid
if (dbIsValid(db)) {
print("SQL database connection is active and ready.")
} else {
stop("Failed to establish a connection to the database.")
}
# Define the function to retrieve run details, including reference genome information
getRunDetails <- function(run_id) {
query <- sprintf("
SELECT
r.run_id,
r.run_date,
r.status,
r.sample_id,
s.collection_date,
s.source,
a.analysis_type,
a.result,
m.key,
m.value AS metadata_value,
rg.name AS genome_name,
rg.version AS genome_version,
sc.component_name,
sc.component_version,
gl.commit_id
FROM
Runs r
JOIN Samples s ON r.sample_id = s.sample_id
JOIN Analyses a ON r.run_id = a.run_id
JOIN Metadata m ON r.run_id = m.run_id
JOIN SystemComponents sc ON r.run_id = sc.run_id
JOIN GitLog gl ON sc.component_id = gl.component_id
LEFT JOIN ReferenceGenomes rg ON m.value = rg.reference_id AND m.key = 'Reference Genome'
WHERE
r.run_id = '%s'", run_id)
dbGetQuery(db, query)
}
# Retrieve run details
run_details <- getRunDetails(run_id)
print(run_details)
# kable(run_details, format="latex", longtable = FALSE, booktabs=FALSE,  position = "H") %>%
# kable_styling(latex_options="scale_down")
readRDS(df, file = "../eurlex_scraper/data/EU_Regulation_Articles.Rds")
print(head(df))
df <- readRDS(file = "../eurlex_scraper/data/EU_Regulation_Articles.Rds")
print(head(df))
df |> dplyr::filter(grepl(article == "art"))
df |> dplyr::filter(grepl("^art", article))
df <- df |> dplyr::filter(grepl("^art", article))
print(head(df))
df <- df |> dplyr::filter(grepl("^art", article)) |> dplyr::select(title, subtitle)
print(head(df))
df <- df |> dplyr::filter(grepl("^art", article)) |> dplyr::select(title, subtitle) |> head()
df <- df |> dplyr::filter(grepl("^art", article)) |> dplyr::select(title, subtitle) |> head()
df <- readRDS(file = "../eurlex_scraper/data/EU_Regulation_Articles.Rds")
df <- df |> dplyr::filter(grepl("^art", article)) |> dplyr::select(title, subtitle) |> head()
print(df)
kable(df)
names(df)
```{r read_article, echo=FALSE, message=FALSE}
df <- readRDS(file = "../eurlex_scraper/data/EU_Regulation_Articles.Rds")
names(df)
df <- df |> dplyr::filter(grepl("^art", article)) |> dplyr::select(title, subtitle, content) |> head()
print(df)
# Wrap the content column to a maximum of 60 characters per line
df$content <- wrap_text(df$content, width = 60)
# Custom function to truncate text at a specified length
truncate_text <- function(text, length = 60) {
sapply(text, function(x) {
if (nchar(x) > length) {
return(paste0(substr(x, 1, length), "..."))
} else {
return(x)
}
})
}
# Truncate the content column to a maximum of 60 characters
df$content <- truncate_text(df$content, length = 60)
kable(df)
# Custom function to remove excess whitespaces
remove_excess_whitespace <- function(text) {
sapply(text, function(x) {
gsub("\\s+", " ", x)
})
}
# Remove excess whitespaces from the content column
df$content <- remove_excess_whitespace(df$content)
kable(df)
library(kableExtra)
library(DBI)
library(knitr)
library(dplyr)
library(igraph)
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=60), width=60)
kable <- function(data) {
knitr::kable(data, format="latex", longtable = FALSE, booktabs=FALSE) %>%
kable_styling(latex_options =c("striped", "scale_down", "HOLD_position"))
}
# Install kableExtra package if you haven't
# if (!requireNamespace("kableExtra", quietly = TRUE))
#   install.packages("kableExtra")
# Define global variables for the database connection to ensure it stays open
run_id <- params$run_id
dbname <- "DNAsnake_IVDR.db"
# Connect to the database
db <- dbConnect(RSQLite::SQLite(), dbname = dbname)
# Ensure the database connection is valid
if (dbIsValid(db)) {
print("SQL database connection is active and ready.")
} else {
stop("Failed to establish a connection to the database.")
}
# Define the function to retrieve run details, including reference genome information
getRunDetails <- function(run_id) {
query <- sprintf("
SELECT
r.run_id,
r.run_date,
r.status,
r.sample_id,
s.collection_date,
s.source,
a.analysis_type,
a.result,
m.key,
m.value AS metadata_value,
rg.name AS genome_name,
rg.version AS genome_version,
sc.component_name,
sc.component_version,
gl.commit_id
FROM
Runs r
JOIN Samples s ON r.sample_id = s.sample_id
JOIN Analyses a ON r.run_id = a.run_id
JOIN Metadata m ON r.run_id = m.run_id
JOIN SystemComponents sc ON r.run_id = sc.run_id
JOIN GitLog gl ON sc.component_id = gl.component_id
LEFT JOIN ReferenceGenomes rg ON m.value = rg.reference_id AND m.key = 'Reference Genome'
WHERE
r.run_id = '%s'", run_id)
dbGetQuery(db, query)
}
# Retrieve run details
run_details <- getRunDetails(run_id)
print(run_details)
# kable(run_details, format="latex", longtable = FALSE, booktabs=FALSE,  position = "H") %>%
# kable_styling(latex_options="scale_down")
df <- readRDS(file = "../eurlex_scraper/data/EU_Regulation_Articles.Rds")
# df <- df |> dplyr::filter(grepl("^art", article)) |> dplyr::select(title, subtitle, content) |> head()
df <- df |> dplyr::filter(grepl("^art", article)) |> dplyr::select(title, content) |> head(20)
# Custom function to remove excess whitespaces
remove_excess_whitespace <- function(text) {
sapply(text, function(x) {
gsub("\\s+", " ", x)
})
}
# Remove excess whitespaces from the content column
df$content <- remove_excess_whitespace(df$content)
# Custom function to truncate text at a specified length
truncate_text <- function(text, length = 60) {
sapply(text, function(x) {
if (nchar(x) > length) {
return(paste0(substr(x, 1, length), "..."))
} else {
return(x)
}
})
}
# Truncate the content column to a maximum of 60 characters
df$content <- truncate_text(df$content, length = 60)
kable(df)
df$linked_to_network <- "No"
df_tail <- df |> tail(10)
df <- df |> head(20)
df <- readRDS(file = "../eurlex_scraper/data/EU_Regulation_Articles.Rds")
df$linked_to_network <- "No"
# df <- df |> dplyr::filter(grepl("^art", article)) |> dplyr::select(title, subtitle, content) |> head()
df <- df |> dplyr::filter(grepl("^art", article)) |> dplyr::select(title, content)
# Custom function to remove excess whitespaces
remove_excess_whitespace <- function(text) {
sapply(text, function(x) {
gsub("\\s+", " ", x)
})
}
# Remove excess whitespaces from the content column
df$content <- remove_excess_whitespace(df$content)
# Custom function to truncate text at a specified length
truncate_text <- function(text, length = 80) {
sapply(text, function(x) {
if (nchar(x) > length) {
return(paste0(substr(x, 1, length), "..."))
} else {
return(x)
}
})
}
# Truncate the content column to a maximum of 60 characters
df$content <- truncate_text(df$content, length = 60)
# display
df_tail <- df |> tail(10)
df <- df |> head(20)
kable(df)
kable(df_tail)
View(df_tail)
row.names(df)
row.names(df_tail)
kable(df_tail, row.names = NULL)
knitr::kable(data, format="latex", longtable = FALSE, booktabs=FALSE,  row.names = NULL) %>%
kable_styling(latex_options =c("striped", "scale_down", "HOLD_position"))
library(kableExtra)
library(DBI)
library(knitr)
library(dplyr)
library(igraph)
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=60), width=60)
kable <- function(data) {
knitr::kable(data, format="latex", longtable = FALSE, booktabs=FALSE) %>%
kable_styling(latex_options =c("striped", "scale_down", "HOLD_position"))
}
# Install kableExtra package if you haven't
# if (!requireNamespace("kableExtra", quietly = TRUE))
#   install.packages("kableExtra")
# Define global variables for the database connection to ensure it stays open
run_id <- params$run_id
dbname <- "DNAsnake_IVDR.db"
# Connect to the database
db <- dbConnect(RSQLite::SQLite(), dbname = dbname)
# Ensure the database connection is valid
if (dbIsValid(db)) {
print("SQL database connection is active and ready.")
} else {
stop("Failed to establish a connection to the database.")
}
# Define the function to retrieve run details, including reference genome information
getRunDetails <- function(run_id) {
query <- sprintf("
SELECT
r.run_id,
r.run_date,
r.status,
r.sample_id,
s.collection_date,
s.source,
a.analysis_type,
a.result,
m.key,
m.value AS metadata_value,
rg.name AS genome_name,
rg.version AS genome_version,
sc.component_name,
sc.component_version,
gl.commit_id
FROM
Runs r
JOIN Samples s ON r.sample_id = s.sample_id
JOIN Analyses a ON r.run_id = a.run_id
JOIN Metadata m ON r.run_id = m.run_id
JOIN SystemComponents sc ON r.run_id = sc.run_id
JOIN GitLog gl ON sc.component_id = gl.component_id
LEFT JOIN ReferenceGenomes rg ON m.value = rg.reference_id AND m.key = 'Reference Genome'
WHERE
r.run_id = '%s'", run_id)
dbGetQuery(db, query)
}
# Retrieve run details
run_details <- getRunDetails(run_id)
print(run_details)
# kable(run_details, format="latex", longtable = FALSE, booktabs=FALSE,  position = "H") %>%
# kable_styling(latex_options="scale_down")
# display
df_tail <- df[103:113, ]
df <- df[1:10, ]
